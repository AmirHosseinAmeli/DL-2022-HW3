یکی از مشکلاتی که
\lr{transformer}
ها دارند این است که مرتبه هزینه محاسباتی و هزینه ذخیره‌سازی عملیات
\lr{self-attention}
دارای عبارت
$N^2$
می‌باشد. این مرتبه باعث می‌شود که آموزش این شبکه روی داده‌های طولانی مانند کتاب مشکل‌زا باشد. دلیل این امر عملگر
\lr{Softmax}
می‌باشد که برای محاسبه شباهت دو بردار استفاده می‌شود. در این تمرین قصد داریم به بررسی یک راهکار جایگزین برای این مورد بپردازیم. یکی از این راهکار‌ها استفاده از مکانیزم‌های توجهی کرنلی می‌باشد.

اگر ورودی را
$x \in {\mathbb{R}^{N \times F}}$
و ماتریس‌های مکانیزم توجه را
${W_Q} \in {\mathbb{R}^{F \times D}},{W_K} \in {\mathbb{R}^{F \times D}}$
و
${W_v} \in {\mathbb{R}^{F \times M}}$
در نظر بگیریم. می‌توان این عملیات را به صورت زیر نوشت:
$$
\begin{array}{l}
	Q = x{W_Q},K = x{W_K},V = x{W_V}\\
	V' = softmax \left( {\frac{{Q{K^T}}}{{\sqrt D }}} \right)V
\end{array}
 $$
 
 حال با تعریف
 $sim\left( {{Q_i},{K_j}} \right) = \exp \left( {\frac{{Q_i^T{K_j}}}{{\sqrt D }}} \right)$
 می‌توان این عبارت را به فرم زیر بازنویسی کرد:
 
\begin{equation} \label{softmax}
	{V_i}^\prime  = \frac{{\sum\limits_{j = 1}^N {sim\left( {{Q_i},{K_j}} \right){V_j}} }}{{\sum\limits_{j = 1}^N {sim\left( {{Q_i},{K_j}} \right)} }}
\end{equation}
 
\begin{enumerate}[label=(\alph*)]
	\item
	مرتبه زمانی و حافظه مورد نیاز برای محاسبه عملگر
	\lr{self-attention}
	بالا را براساس پارامتر‌های
	\lr{N,D,M}
	محاسبه کنید.
	
	\item
	یکی از توابعی که می‌توان جایگزین
	$sim\left( {{Q_i},{K_j}} \right)$
	کرد، کرنل توجه چند‌جمله‌ای می‌باشد. عبارت جایگزین را برای حالت درجه دو
	\lr{(Quadratic)}
	بنویسید.
	\item
	برای کرنل مرتبه بخش قبل، بردار ویژگی
	$\phi \left(  \cdot  \right)$
	را بنویسید
	\item
	حال باتوجه به رابطه
	${\rm K}\left( {q,k} \right) = \phi {\left( q \right)^T}\phi \left( k \right)$
	، رابطه
	\ref{softmax}
	را بازنویسی کنید و مرتبه زمانی رابطه و مرتبه حافظه مورد نیاز را برای آن محاسبه کنید. با مقایسه این مرتبه‌ها با مرتبه‌های رابطه قبلی، در چه شرایطی استفاده از این رابطه بهتر از رابطه قبلی می‌باشد؟
	
\end{enumerate}