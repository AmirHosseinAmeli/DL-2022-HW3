\begin{enumerate}[label=(\alph*)]
	
	\item
	فرض کنید ورودی
	$x \in \mathbb{R}{^D}$
	به انکودر داده شده و z به دست آمده است. خروجی شبکه دیکودر را به صورت
	$a\left( z \right) = \sigma \left( {f\left( z \right)} \right) \in \mathbb{R}{^D}$
	در نظر میگیریم و خروجی
	${\hat x}$
	را با نمونه برداری از توزیع
	$a\left( z \right)$
	میسازیم. در این صورت احتمال آن که در خروجی شبکه دیکودر پس از نمونه برداری
	$x$
	 به دست آمده باشد را میتوان با
	 ${P_\theta }\left( {\hat x = x|z} \right)$
	 نمایش داد. حال اگر برای
	 ${\hat x}$
	 یک توزیع برنولی چند متغیره در نظر بگیریم، میتوان این احتمال را به صورت زیر نوشت:
	 $$
	 {P_\theta }\left( {\hat x = x|z} \right) = \prod\limits_{i = 1}^D {{{\left( {{a_i}} \right)}^{{x_i}}}{{\left( {1 - {a_i}} \right)}^{1 - {x_i}}}}
	 $$
	 
	 رابطه فوق چیزی نیست به جز احتمال مشترک چند متغیر برنولی که در آن
	 ${{a_i}}$
	 درایه i ام بردار خروجی
	 $a\left( z \right)$
	 میباشد. با توجه به این توضیحات، میتوان لگاریتم این عبارت را به صورت زیر نوشت:
	 
	 $$
	 \log {P_\theta }\left( {\hat x = x|z} \right) = \log \left[ {\prod\limits_{i = 1}^D {{{\left( {{a_i}} \right)}^{{x_i}}}{{\left( {1 - {a_i}} \right)}^{1 - {x_i}}}} } \right] = \sum\limits_{i = 1}^D {{x_i}\log {a_i} + (1 - {x_i})\log (1 - {a_i})}  =  - BCE\left( {x,a\left( z \right)} \right)
	 $$
	 
	 \item
	 
	 راه حلی که برای این مشکل پیشنهاد میشود، روش Gumble-Softmax است که یک روش تقریبی میباشد. این روش در چند گام انجام میشود که باعث میشود اولا اپراتور احتمالاتی از مسیر اصلی گراف محاسبات کنار رفته و محاسبات به صورت deterministic انجام شود و ثانیا با کنار گذاشتن اپراتورهای گسسته از مسیر اصلی گراف، بتوان backprop  را به درستی انجام داد. برای این منظور فرض کنید یک مسئله نمونه برداری k کلاسه داریم و میخواهیم از توزیع داده شده نمونه بگیریم. همچنین فرض کنید logits هایی که میخواهیم از آن ها نمونه بگیریم را با
	 $\left\{ {{a_i}} \right\}_{i = 1}^k$
	 نشان دهیم. در این صورت این الگوریتم پیشنهاد میکند تا گامهای زیر طی شود:
	 
	 \begin{itemize}
	 	\item
	 	در گام نخست k متغیر
	 	$\left\{ {{k_i}} \right\}_{i = 1}^k$
	 	را به صورت مستقل از توزیع
	 	$uniform(0,1)$
	 	نمونه میگیریم.
	 	
	 	\item
	 	متغیرهای
	 	$\left\{ {{g_i}} \right\}_{i = 1}^k$
	 	را به صورت
	 	${g_i} =  - \log \left( { - \log \left( {{u_i}} \right)} \right)$
	 	تشکیل میدهیم. در این صورت هر یک از متغیرهای
	 	$g_i$
	 	یک توزیع Gumbel	 استاندارد دارند.
	 	
	 	\item
	 	
	 	تا این جا شاخه احتمالاتی تولید متغیر را از گراف اصلی جدا کردیم حالا لازم است تا این متغیرها را با logit هایی که از قبل داشتیم  ترکیب کنیم. برای این منظور جملات
	 	$b_i$
	 	را به صورت
	 	${b_i} = {a_i} + {g_i}$
	 	میسازیم. میتوان نشان داد که متغیر تصادفی
	 	$j = \arg \mathop {\max }\limits_i {b_i}$
	 	دقیقاً همان توزیعی را دارد که ما به دنبال آن بودیم. اما هنوز یک مشکل دیگر باقی مانده و آن استفاده از اپراتور گسسته argmax در وسط شبکه است.
	 	
	 	\item
	 	
	 	برای حل مشکل این اپراتور گسسته، از یک روش تقریبی با کمک لایه Softmax استفاده میکنند. لایه Softmax علاوه بر بردار توزیع احتمالات ورودی، یک پارامتر دیگر نیز به عنوان ورودی دریافت میکند که دما (Temperature) نام دارد. نحوه اعمال این پارامتر روی ورودی به این صورت است که ابتدا همه دادههای ورودی به این پارامتر دما
	 	$\left( \lambda  \right)$
	 	تقسیم میشوند و سپس از Softmax عادی استفاده میشود. پارامتر دما، اثر ویژهای روی شکل بردار خروجی دارد؛ اگر 
	 	$\lambda  = 1$
	 	باشد که همان سافتمکس معمولی خواهد بود اما هرچه
	 	$\lambda$
	 	به صفر نزدیکتر شود، بردار به دست آمده در خروجی به یک بردار one-hot نزدیکتر میشود. در واقع با کوچک شدن این پارامتر به اندازه کافی، تنها درایه با بیشترین مقدار در ورودی برابر یک شده و بقیه به صفر خیلی نزدیک میشوند. همچنین با زیاد شدن
	 	$\lambda$
	 	به سمت بینهایت، مستقل از توزیع ورودی، یک توزیع یکنواخت در خروجی خواهیم داشت. لذا مطلوب ترین حالت همان است که
	 	$\lambda$
	 	تا جای ممکن کوچک انتخاب شود تا تابع argmax به خوبی تقریب زده شود. البته باید توجه کرد که خیلی کوچک گرفتن پارامتر دما میتواند باعث زیاد شدن واریانس گرادیان بازگشتی از این لایه شود.
	 	
	 	
 	\end{itemize}
 
 	\item
 	
 	در
 	$\beta VAE$
 	دقیقاً مشابه
 	$VAE$
 	هدف یادگرفتن فضای نهانی است که بتواند دادها را خوب تولید کند با این تفاوت که در
 	$\beta VAE$
 	تاکید بیشتری روی disentangle بودن فضای نهان صورت میگیرد. به طور دقیقتر،
 	$\beta VAE$
 	نیز همانند
 	$VAE$
 	به دنبال بهینه کردن خطای بازسازی به صورت زیر است:
 	$$
 	\mathop {\max }\limits_{\theta ,\varphi } {\mathbb{E}_{x \sim {P_{Data}}}}\left[ {{\mathbb{E}_{z \sim {q_\varphi }\left( {z|x} \right)}}\left[ {\log {P_\theta }\left( {x|z} \right)} \right]} \right]
 	$$
 	علاوه بر عبارت فوق، لازم است تا یک شرط دیگر به منظور ساده کردن فضای نهان اضافه کنیم. به عبارت دیگر مسئله بهینه سازی
 	$\beta VAE$
 	یک مسئله constrained است که شرط آن روی فضای نهان به صورت زیر خواهد بود:
 	$$
 	\begin{array}{l}
 		\mathop {\max }\limits_{\theta ,\varphi } {\mathbb{E}_{x \sim {P_{Data}}}}\left[ {{\mathbb{E}_{z \sim {q_\varphi }\left( {z|x} \right)}}\left[ {\log {P_\theta }\left( {x|z} \right)} \right]} \right]\\
 		KL\left( {{q_\varphi }\left( {z|x} \right)||P\left( z \right)} \right) < \delta  \Rightarrow KL\left( {{q_\varphi }\left( {z|x} \right)||P\left( z \right)} \right) - \delta  < 0
 	\end{array}
 	$$
 	مسئله فوق یک مسئله بهینه سازی مشروط است که حل کردن آن الزاماً ساده نیست. برای از بین بردن شرط، از تکنیک لاگرانژ و ضریب
 	$\beta$
 	(به عنوان یک هایپرپارمتر) استفاده میشود و قسمت شرط را وارد عبارت بهینه سازی میکنند:
 	$$
 	{\mathbb{E}_{x \sim {P_{Data}}}}\left[ {{\mathbb{E}_{z \sim {q_\varphi }\left( {z|x} \right)}}\left[ {\log {P_\theta }\left( {x|z} \right)} \right]} \right] - \beta \left( {KL\left( {{q_\varphi }\left( {z|x} \right)||P\left( z \right)} \right) - \delta } \right)
 	$$
 	حال توجه کنید که عبارت
 	$\beta \delta$
 	یک مقدار مثبت ثابت و مستقل از پارامترهاست لذا در مجموع میتوان تابع ضرر زیر را برای شبکه نوشت که به عنوان تابع ضرر
 	$\beta VAE$
 	شناخته می‌شود:
 	$$
 	L\left( {\theta ,\varphi } \right) =  - {\mathbb{E}_{x \sim {P_{Data}}}}\left[ {{\mathbb{E}_{z \sim {q_\varphi }\left( {z|x} \right)}}\left[ {\log {P_\theta }\left( {x|z} \right)} \right]} \right] + \beta KL\left( {{q_\varphi }\left( {z|x} \right)||P\left( z \right)} \right)
 	$$
\end{enumerate}